{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Aprenentatge profund per a imatges\n",
    "\n",
    "A les lliçons anteriors, hem treballat extensament amb **dades tabulars**. Aquests conjunts de dades són generalment fàcils de representar i analitzar utilitzant eines com `pandas`, `scikit-learn` i diversos models estadístics.\n",
    "\n",
    "Tanmateix, les **imatges** són fonamentalment diferents tant en estructura com en contingut. En lloc de files i columnes amb característiques etiquetades explícitament, una imatge és una graella de valors de píxels, normalment en 2D per a escala de grisos o en 3D per a color (alçada × amplada × canals). Per exemple, una imatge en color de mida 224×224 píxels amb 3 canals de color (RGB) conté més de 150.000 valors en brut, cap dels quals no està etiquetat amb característiques interpretables per humans com “edat” o “ingrés”.\n",
    "\n",
    "Aquesta diferència condueix a diversos reptes importants:\n",
    "\n",
    "- **Alta dimensionalitat**: Les imatges contenen moltes més característiques (píxels) que els conjunts de dades tabulars típics, cosa que augmenta la complexitat computacional i el risc de *overfitting*.\n",
    "- **Estructura espacial**: Els píxels propers en una imatge sovint estan relacionats, formant vores, textures i patrons. Els models tabulars generalment no capten aquestes dependències locals.\n",
    "- **Bretxa semàntica**: La relació entre els píxels en brut i els conceptes significatius (com un moic, una cara o un senyal de trànsit) és complexa i no lineal, i requereix models sofisticats per salvar aquesta bretxa.\n",
    "\n",
    "\n",
    "En aquest mòdul explorarem com treballar amb dades d’imatges Per fer-ho, tornarem a utilitzar `PyTorch`.\n",
    "\n",
    "## *Batches*\n",
    "\n",
    "Quan entrenem models d’aprenentatge profund amb imatges, no alimentem tot el conjunt de dades alhora, com fèiem amb les dades tabulars. En lloc d’això, agrupem múltiples imatges en *batches*.\n",
    "\n",
    "Un *batch* és una col·lecció de mostres (p. ex. imatges i les seves etiquetes) processades juntes en una sola passada endavant i enrere. Com a resultat les dades d'imatge són representades com un tensor de 4 dimensions:\n",
    "\n",
    "```(batch_size, channels, height, width)\n",
    "```\n",
    "\n",
    "## El conjunt de dades MNIST\n",
    "\n",
    "Per començar a treballar amb dades d’imatges en la pràctica, utilitzarem un dels conjunts de dades de referència més coneguts: **MNIST**. **MNIST** significa *Modified National Institute of Standards and Technology*. Consta de **70.000 imatges en escala de grisos** de xifres manuscrites (del 0 al 9), dividides en:\n",
    "\n",
    "- **60.000 imatges d’entrenament**\n",
    "- **10.000 imatges de prova**\n",
    "\n",
    "Cada imatge és:\n",
    "\n",
    "- **28 × 28 píxels**\n",
    "- **En escala de grisos** (és a dir, un sol canal)\n",
    "- **Etiquetada** amb la xifra correcta (0–9)\n",
    "\n",
    "El conjunt de dades MNIST ha estat àmpliament utilitzat com a banc de proves per a algorismes d’aprenentatge automàtic i aprenentatge profund. Aquest conjunt de dades és ideal per aprendre perquè és petit, net i ja preprocesat, però alhora ofereix una complexitat realista en les xifres manuscrites.\n",
    "\n",
    "### Exemple d’imatges MNIST\n",
    "\n",
    "A continuació es mostra una mostra de xifres MNIST:\n",
    "\n",
    "![MNIST Examples](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "Cada fila a la imatge anterior mostra les xifres del 0 al 9 escrites per persones diferents. Com pots veure, algunes xifres s’escriuen de maneres molt diferents, i és per això que necessitem l’aprenentatge automàtic per reconèixer-les automàticament."
   ],
   "id": "836011f370280568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T09:11:29.413260Z",
     "start_time": "2025-10-08T09:11:19.762407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "id": "f938c9d912b0d9b6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Carrega de dades\n",
    "\n",
    "El processament de dades, com ja hem vist, és una part fonamental de qualsevol projecte d'aprenentatge profund. Per treballar amb dades de manera eficient, PyTorch ofereix dues classes: ``Dataset`` i ``DataLoader``.\n",
    "\n",
    "El ``Dataset`` és una classe que representa un conjunt de dades. Serveix per carregar, transformar i accedir als elements individuals del conjunt. PyTorch inclou diversos datasets predefinits com MNIST, CIFAR-10 o ImageNet, però també podem crear els nostres propis datasets personalitzats heretant de torch.utils.data.Dataset i implementant els mètodes __len__() i __getitem__().\n",
    "\n",
    "Un cop tenim el dataset, el ``DataLoader`` s’encarrega de gestionar la manera com aquestes dades s’entreguen al model. Permet dividir les dades en **batches**, barrejar-les (shuffle) i carregar-les en paral·lel utilitzant múltiples fils (*workers*)."
   ],
   "id": "73205d6e94fde810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T09:15:45.047486Z",
     "start_time": "2025-10-08T09:15:33.165848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_train = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "dataset_val = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE)"
   ],
   "id": "9f1e9c193199ef4b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:05<00:00, 1.65MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 532kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.17MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T09:16:03.532051Z",
     "start_time": "2025-10-08T09:16:03.487030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for img, gt in dataset_train:\n",
    "\tprint(img.shape)\n",
    "\tprint(gt)\n",
    "\tbreak"
   ],
   "id": "53db4a8a72fa186c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T09:16:05.814860Z",
     "start_time": "2025-10-08T09:16:05.757637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch, gt in dataloader_train:\n",
    "\tprint(batch.shape)\n",
    "\tprint(gt.shape)\n",
    "\tbreak"
   ],
   "id": "973459a1d81507dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`nn.Sequential`` és una manera senzilla de construir una xarxa neuronal apilant les capes en ordre.\n",
    "\n",
    "És útil quan el teu model és una cadena lineal de capes, sense ramificacions ni lògica personalitzada."
   ],
   "id": "f6751c5d3079d7df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T09:16:08.390643Z",
     "start_time": "2025-10-08T09:16:08.371395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlp_net = nn.Sequential(\n",
    "    torch.nn.Linear(784, 10),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(10, 10)\n",
    ")"
   ],
   "id": "8336cc39da8f39e7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenament",
   "id": "b19b1715c1af2d0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T09:25:45.925441Z",
     "start_time": "2025-10-08T09:25:45.916107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(mlp_net.parameters(), lr=LR)"
   ],
   "id": "649560387e03ee96",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T09:27:22.870309Z",
     "start_time": "2025-10-08T09:25:47.420271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "running_loss = []\n",
    "running_acc = []\n",
    "\n",
    "running_test_loss = []\n",
    "running_test_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "for t in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    i_batch = 0\n",
    "    for i_batch, (x, y) in enumerate(dataloader_train):  # We have to iter the batches.\n",
    "        mlp_net.train()\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten images\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mlp_net(x)\n",
    "\n",
    "        # 1. LOSS CALCULATION\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # 2. GRADIENT\n",
    "        mlp_net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # 3. OPTIMISATION\n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "        # 4. EVALUATION\n",
    "        mlp_net.eval()  # Mode avaluació de la xarxa\n",
    "\n",
    "        y_pred = mlp_net(x)\n",
    "        y_pred_binary = torch.argmax(y_pred, 1).double()\n",
    "\n",
    "        batch_loss += (loss_fn(y_pred, y).detach())\n",
    "        batch_acc += accuracy_score(y_pred_binary.detach(), y.detach())\n",
    "\n",
    "    running_loss.append(batch_loss / (i_batch + 1))\n",
    "    running_acc.append(batch_acc / (i_batch + 1))\n",
    "    \n",
    "    # --- VALIDACIÓ ---\n",
    "    mlp_net.eval()\n",
    "    val_batch_loss = 0\n",
    "    val_batch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in dataloader_val:\n",
    "            x_val = x_val.reshape(x_val.shape[0], -1)\n",
    "            y_pred_val = mlp_net(x_val)\n",
    "            loss_val = loss_fn(y_pred_val, y_val)\n",
    "            y_pred_val_binary = torch.argmax(y_pred_val, 1)\n",
    "\n",
    "            val_batch_loss += loss_val.item()\n",
    "            val_batch_acc += accuracy_score(y_pred_val_binary, y_val)\n",
    "\n",
    "    val_loss.append(val_batch_loss / len(dataloader_val))\n",
    "    val_acc.append(val_batch_acc / len(dataloader_val))\n",
    "\n",
    "    print(f\"Epoch {t+1}/{EPOCHS} - \"\n",
    "          f\"Train loss: {running_loss[-1]:.4f}, acc: {running_acc[-1]*100:.2f}% | \"\n",
    "          f\"Val loss: {val_loss[-1]:.4f}, acc: {val_acc[-1]*100:.2f}%\")"
   ],
   "id": "3d18c723b9f345c0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 1/5 [00:19<01:19, 19.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train loss: 0.2783, acc: 92.17% | Val loss: 0.2785, acc: 92.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 2/5 [00:38<00:57, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train loss: 0.2698, acc: 92.33% | Val loss: 0.2727, acc: 92.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 3/5 [00:57<00:38, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train loss: 0.2637, acc: 92.51% | Val loss: 0.2760, acc: 92.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 4/5 [01:16<00:19, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train loss: 0.2570, acc: 92.74% | Val loss: 0.2694, acc: 92.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [01:35<00:00, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train loss: 0.2517, acc: 92.87% | Val loss: 0.2682, acc: 92.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tasca a fer\n",
    "\n",
    "1. Seleccionar la funció de pèrdua a emprar per poder entrenar el model.\n",
    "2. Afegir la validació al bucle del `MNIST` i contestar a la pregunta de si hi ha *overfitting*?\n",
    "2. Crear dos objectes `DataLoader` pel següent [dataset](https://github.com/miquelmn/aa_2526/releases/download/pr3/p4.tar.gz). Per fer-ho haureu de primer descarregar les imatges de l'enllaç, i després carregar les dades emprant [ImageFolder](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) de `PyTorch`.\n",
    "3. Entrenar un **MLP** amb `PyTorch` per tal d'identificar les classes. Prova primer amb una mida d'imatges més petita (64 per 64 píxels) fins a la mida original."
   ],
   "id": "be00057761220c69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:18:12.921778Z",
     "start_time": "2025-10-08T10:18:12.849391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "img_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=\"train\", transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(root=\"test\", transform=transform)\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Classes trobades:\", train_dataset.classes)\n",
    "print(\"Nombre d'imatges d'entrenament:\", len(train_dataset))\n",
    "print(\"Nombre d'imatges de test:\", len(test_dataset))"
   ],
   "id": "ede2db9f316e1358",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes trobades: ['0', '4']\n",
      "Nombre d'imatges d'entrenament: 2000\n",
      "Nombre d'imatges de test: 20\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:18:15.153806Z",
     "start_time": "2025-10-08T10:18:15.111969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = img_size * img_size  # 64*64 = 4096\n",
    "hidden_size1 = 512\n",
    "hidden_size2 = 128\n",
    "output_size = len(train_dataset.classes)  # 2 classes: 0 i 4\n",
    "\n",
    "mlp_net = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(hidden_size1, hidden_size2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size2, output_size)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_net.parameters(), lr=1e-3)\n",
    "epochs = 5\n"
   ],
   "id": "2b32c644aec3bd3a",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:18:50.018984Z",
     "start_time": "2025-10-08T10:18:17.728932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "for t in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    for x, y in dataloader_train:\n",
    "        mlp_net.train()\n",
    "        x = x.reshape(x.shape[0], -1)   # aplanar\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mlp_net(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        mlp_net.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "        \n",
    "        mlp_net.eval()\n",
    "\n",
    "        y_pred = mlp_net(x)\n",
    "        y_pred_binary = torch.argmax(y_pred, 1).double()\n",
    "        \n",
    "        batch_loss += (loss_fn(y_pred, y).detach())\n",
    "        batch_acc += accuracy_score(y_pred_binary.detach(), y.detach())\n",
    "\n",
    "    print(f\"Epoch {t+1}/{epochs} - \"\n",
    "          f\"Train loss: {batch_loss/len(dataloader_train):.4f}, \"\n",
    "          f\"acc: {batch_acc/len(dataloader_train)*100:.2f}%\")\n",
    "    \n",
    "    "
   ],
   "id": "13391cacfddd3e76",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 1/5 [00:07<00:30,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train loss: 0.7236, acc: 50.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 2/5 [00:13<00:20,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train loss: 0.7027, acc: 50.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 3/5 [00:19<00:12,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train loss: 0.6986, acc: 50.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 4/5 [00:25<00:06,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train loss: 0.6939, acc: 50.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [00:32<00:00,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train loss: 0.6927, acc: 51.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
