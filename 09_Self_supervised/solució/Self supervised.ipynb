{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Self-supervised learning amb Autoencoders",
   "id": "c9c0a24df40d6d3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:49:23.975632Z",
     "start_time": "2025-11-19T09:49:11.055205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "e3390ba7dc0e564",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bielp\\PycharmProjects\\ia_2024\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definim variables globals",
   "id": "cf9821987fbf256c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:51:34.267630Z",
     "start_time": "2025-11-19T09:51:34.256989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"../archive/caltech-101\"\n",
    "save_dir = \"./reconstructions\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "image_size = (128, 128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "8a2a6324fbff6d80",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cream el dataset amb imatges rotades\n",
    "\n",
    "Cream un dataset personalitzat que carrega imatges des d'una carpeta i les retorna juntament amb una versió rotada de la mateixa imatge. Per fer-ho utilitzem la classe `ImageFolder` de `torchvision.datasets` per carregar les imatges i apliquem una rotació aleatòria a cada imatge durant la recuperació de l'element del dataset."
   ],
   "id": "843c8b86d078b0de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:49:51.624087Z",
     "start_time": "2025-11-19T09:49:51.603149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RotatedImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(root=root, transform=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.dataset[index]\n",
    "\n",
    "        angle = random.choice((90, 180, 270))\n",
    "        target = img.copy()\n",
    "\n",
    "        target, inp = self.transform(img), self.transform(target)\n",
    "\n",
    "        if angle != 0:\n",
    "            inp = transforms.functional.rotate(target, angle)\n",
    "\n",
    "\n",
    "        return inp, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ],
   "id": "aecd2c42808a829c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dividim el dataset en train i test\n",
    "\n",
    "El *dataset* complet es divideix en conjunts d'entrenament i de prova utilitzant `train_test_split` de `sklearn.model_selection`. Una vegada dividit, es creen subconjunts utilitzant `torch.utils.data.Subset` i es carreguen en *DataLoaders* per a l'entrenament."
   ],
   "id": "fa9c7d18766e3ecb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:51:39.013334Z",
     "start_time": "2025-11-19T09:51:37.624845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whole_dataset = RotatedImageDataset(\n",
    "    root=data_dir,\n",
    "    transform=input_transform,\n",
    ")\n",
    "\n",
    "\n",
    "idx_datasets = np.arange(len(whole_dataset))\n",
    "train_dss, test_dss = train_test_split(idx_datasets)\n",
    "\n",
    "train_ds = torch.utils.data.Subset(whole_dataset, train_dss)\n",
    "test_ds = torch.utils.data.Subset(whole_dataset, test_dss)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ],
   "id": "21c045114cb8d215",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definició de l'Autoencoder",
   "id": "418535abec4e8b9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:51:42.409590Z",
     "start_time": "2025-11-19T09:51:42.395091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "id": "8c7854dc964b6427",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "6ed29316-819e-490d-87fd-c88700743257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:51:45.876758Z",
     "start_time": "2025-11-19T09:51:45.772869Z"
    }
   },
   "source": [
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Entrenament de l'Autoencoder",
   "id": "e6060cf786b87c9"
  },
  {
   "cell_type": "code",
   "id": "d2240c0c-d50f-49b2-ac5d-4bf176bf9aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T09:56:54.023497Z",
     "start_time": "2025-11-19T09:51:48.087581Z"
    }
   },
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in tqdm(train_loader, leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        save_image(outputs[:8], f\"{save_dir}/recon_epoch_{epoch+1}.png\")\n",
    "        save_image(inputs[:8], f\"{save_dir}/inputs_epoch_{epoch+1}.png\")\n",
    "        save_image(targets[:8], f\"{save_dir}/targets_epoch_{epoch+1}.png\")\n",
    "\n",
    "print(\"Training complete! Reconstructed images saved in\", save_dir)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "  0%|          | 0/215 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/215 [00:02<08:48,  2.47s/it]\u001B[A\n",
      "  1%|          | 2/215 [00:03<06:35,  1.86s/it]\u001B[A\n",
      "  1%|▏         | 3/215 [00:05<05:56,  1.68s/it]\u001B[A\n",
      "  2%|▏         | 4/215 [00:06<05:36,  1.60s/it]\u001B[A\n",
      "  2%|▏         | 5/215 [00:08<05:18,  1.52s/it]\u001B[A\n",
      "  3%|▎         | 6/215 [00:09<05:08,  1.48s/it]\u001B[A\n",
      "  3%|▎         | 7/215 [00:10<04:54,  1.42s/it]\u001B[A\n",
      "  4%|▎         | 8/215 [00:12<04:52,  1.41s/it]\u001B[A\n",
      "  4%|▍         | 9/215 [00:13<04:48,  1.40s/it]\u001B[A\n",
      "  5%|▍         | 10/215 [00:15<04:44,  1.39s/it]\u001B[A\n",
      "  5%|▌         | 11/215 [00:16<04:45,  1.40s/it]\u001B[A\n",
      "  6%|▌         | 12/215 [00:17<04:44,  1.40s/it]\u001B[A\n",
      "  6%|▌         | 13/215 [00:19<04:44,  1.41s/it]\u001B[A\n",
      "  7%|▋         | 14/215 [00:20<04:43,  1.41s/it]\u001B[A\n",
      "  7%|▋         | 15/215 [00:22<04:39,  1.40s/it]\u001B[A\n",
      "  7%|▋         | 16/215 [00:23<04:34,  1.38s/it]\u001B[A\n",
      "  8%|▊         | 17/215 [00:24<04:33,  1.38s/it]\u001B[A\n",
      "  8%|▊         | 18/215 [00:26<04:34,  1.39s/it]\u001B[A\n",
      "  9%|▉         | 19/215 [00:27<04:35,  1.41s/it]\u001B[A\n",
      "  9%|▉         | 20/215 [00:29<04:35,  1.41s/it]\u001B[A\n",
      " 10%|▉         | 21/215 [00:30<04:36,  1.43s/it]\u001B[A\n",
      " 10%|█         | 22/215 [00:31<04:35,  1.43s/it]\u001B[A\n",
      " 11%|█         | 23/215 [00:33<04:30,  1.41s/it]\u001B[A\n",
      " 11%|█         | 24/215 [00:34<04:28,  1.40s/it]\u001B[A\n",
      " 12%|█▏        | 25/215 [00:36<04:28,  1.41s/it]\u001B[A\n",
      " 12%|█▏        | 26/215 [00:37<04:31,  1.44s/it]\u001B[A\n",
      " 13%|█▎        | 27/215 [00:39<04:33,  1.45s/it]\u001B[A\n",
      " 13%|█▎        | 28/215 [00:40<04:33,  1.46s/it]\u001B[A\n",
      " 13%|█▎        | 29/215 [00:41<04:25,  1.43s/it]\u001B[A\n",
      " 14%|█▍        | 30/215 [00:43<04:18,  1.40s/it]\u001B[A\n",
      " 14%|█▍        | 31/215 [00:44<04:16,  1.39s/it]\u001B[A\n",
      " 15%|█▍        | 32/215 [00:46<04:12,  1.38s/it]\u001B[A\n",
      " 15%|█▌        | 33/215 [00:47<04:09,  1.37s/it]\u001B[A\n",
      " 16%|█▌        | 34/215 [00:48<04:05,  1.36s/it]\u001B[A\n",
      " 16%|█▋        | 35/215 [00:50<04:02,  1.35s/it]\u001B[A\n",
      " 17%|█▋        | 36/215 [00:51<03:58,  1.33s/it]\u001B[A\n",
      " 17%|█▋        | 37/215 [00:52<04:01,  1.36s/it]\u001B[A\n",
      " 18%|█▊        | 38/215 [00:54<04:02,  1.37s/it]\u001B[A\n",
      " 18%|█▊        | 39/215 [00:55<04:01,  1.37s/it]\u001B[A\n",
      " 19%|█▊        | 40/215 [00:56<03:56,  1.35s/it]\u001B[A\n",
      " 19%|█▉        | 41/215 [00:58<03:54,  1.35s/it]\u001B[A\n",
      " 20%|█▉        | 42/215 [00:59<03:49,  1.33s/it]\u001B[A\n",
      " 20%|██        | 43/215 [01:00<03:51,  1.35s/it]\u001B[A\n",
      " 20%|██        | 44/215 [01:02<03:50,  1.35s/it]\u001B[A\n",
      " 21%|██        | 45/215 [01:03<03:49,  1.35s/it]\u001B[A\n",
      " 21%|██▏       | 46/215 [01:04<03:48,  1.35s/it]\u001B[A\n",
      " 22%|██▏       | 47/215 [01:06<03:48,  1.36s/it]\u001B[A\n",
      " 22%|██▏       | 48/215 [01:07<03:47,  1.36s/it]\u001B[A\n",
      " 23%|██▎       | 49/215 [01:09<03:48,  1.37s/it]\u001B[A\n",
      " 23%|██▎       | 50/215 [01:10<03:48,  1.38s/it]\u001B[A\n",
      " 24%|██▎       | 51/215 [01:11<03:44,  1.37s/it]\u001B[A\n",
      " 24%|██▍       | 52/215 [01:13<03:46,  1.39s/it]\u001B[A\n",
      " 25%|██▍       | 53/215 [01:14<03:44,  1.39s/it]\u001B[A\n",
      " 25%|██▌       | 54/215 [01:15<03:41,  1.37s/it]\u001B[A\n",
      " 26%|██▌       | 55/215 [01:17<03:40,  1.38s/it]\u001B[A\n",
      " 26%|██▌       | 56/215 [01:18<03:38,  1.38s/it]\u001B[A\n",
      " 27%|██▋       | 57/215 [01:20<03:35,  1.36s/it]\u001B[A\n",
      " 27%|██▋       | 58/215 [01:21<03:32,  1.35s/it]\u001B[A\n",
      " 27%|██▋       | 59/215 [01:22<03:32,  1.36s/it]\u001B[A\n",
      " 28%|██▊       | 60/215 [01:24<03:30,  1.36s/it]\u001B[A\n",
      " 28%|██▊       | 61/215 [01:25<03:28,  1.36s/it]\u001B[A\n",
      " 29%|██▉       | 62/215 [01:26<03:26,  1.35s/it]\u001B[A\n",
      " 29%|██▉       | 63/215 [01:28<03:22,  1.33s/it]\u001B[A\n",
      " 30%|██▉       | 64/215 [01:29<03:24,  1.36s/it]\u001B[A\n",
      " 30%|███       | 65/215 [01:30<03:21,  1.34s/it]\u001B[A\n",
      " 31%|███       | 66/215 [01:32<03:20,  1.34s/it]\u001B[A\n",
      " 31%|███       | 67/215 [01:33<03:22,  1.37s/it]\u001B[A\n",
      " 32%|███▏      | 68/215 [01:35<03:26,  1.40s/it]\u001B[A\n",
      " 32%|███▏      | 69/215 [01:36<03:24,  1.40s/it]\u001B[A\n",
      " 33%|███▎      | 70/215 [01:37<03:20,  1.38s/it]\u001B[A\n",
      " 33%|███▎      | 71/215 [01:39<03:15,  1.36s/it]\u001B[A\n",
      " 33%|███▎      | 72/215 [01:40<03:20,  1.40s/it]\u001B[A\n",
      " 34%|███▍      | 73/215 [01:42<03:18,  1.40s/it]\u001B[A\n",
      " 34%|███▍      | 74/215 [01:43<03:18,  1.41s/it]\u001B[A\n",
      " 35%|███▍      | 75/215 [01:44<03:17,  1.41s/it]\u001B[A\n",
      " 35%|███▌      | 76/215 [01:46<03:18,  1.43s/it]\u001B[A\n",
      " 36%|███▌      | 77/215 [01:47<03:14,  1.41s/it]\u001B[A\n",
      " 36%|███▋      | 78/215 [01:49<03:14,  1.42s/it]\u001B[A\n",
      " 37%|███▋      | 79/215 [01:50<03:12,  1.41s/it]\u001B[A\n",
      " 37%|███▋      | 80/215 [01:51<03:09,  1.40s/it]\u001B[A\n",
      " 38%|███▊      | 81/215 [01:53<03:11,  1.43s/it]\u001B[A\n",
      " 38%|███▊      | 82/215 [01:54<03:11,  1.44s/it]\u001B[A\n",
      " 39%|███▊      | 83/215 [01:56<03:10,  1.44s/it]\u001B[A\n",
      " 39%|███▉      | 84/215 [01:57<03:06,  1.43s/it]\u001B[A\n",
      " 40%|███▉      | 85/215 [01:59<03:05,  1.43s/it]\u001B[A\n",
      " 40%|████      | 86/215 [02:00<03:03,  1.42s/it]\u001B[A\n",
      " 40%|████      | 87/215 [02:01<03:01,  1.42s/it]\u001B[A\n",
      " 41%|████      | 88/215 [02:03<02:58,  1.40s/it]\u001B[A\n",
      " 41%|████▏     | 89/215 [02:04<02:56,  1.40s/it]\u001B[A\n",
      " 42%|████▏     | 90/215 [02:06<02:54,  1.40s/it]\u001B[A\n",
      " 42%|████▏     | 91/215 [02:07<02:54,  1.41s/it]\u001B[A\n",
      " 43%|████▎     | 92/215 [02:08<02:49,  1.38s/it]\u001B[A\n",
      " 43%|████▎     | 93/215 [02:10<02:46,  1.37s/it]\u001B[A\n",
      " 44%|████▎     | 94/215 [02:11<02:45,  1.37s/it]\u001B[A\n",
      " 44%|████▍     | 95/215 [02:12<02:43,  1.36s/it]\u001B[A\n",
      " 45%|████▍     | 96/215 [02:14<02:41,  1.35s/it]\u001B[A\n",
      " 45%|████▌     | 97/215 [02:15<02:38,  1.34s/it]\u001B[A\n",
      " 46%|████▌     | 98/215 [02:16<02:37,  1.35s/it]\u001B[A\n",
      " 46%|████▌     | 99/215 [02:18<02:38,  1.37s/it]\u001B[A\n",
      " 47%|████▋     | 100/215 [02:19<02:35,  1.35s/it]\u001B[A\n",
      " 47%|████▋     | 101/215 [02:20<02:34,  1.35s/it]\u001B[A\n",
      " 47%|████▋     | 102/215 [02:22<02:36,  1.38s/it]\u001B[A\n",
      " 48%|████▊     | 103/215 [02:23<02:34,  1.38s/it]\u001B[A\n",
      " 48%|████▊     | 104/215 [02:25<02:34,  1.39s/it]\u001B[A\n",
      " 49%|████▉     | 105/215 [02:26<02:35,  1.41s/it]\u001B[A\n",
      " 49%|████▉     | 106/215 [02:28<02:32,  1.40s/it]\u001B[A\n",
      " 50%|████▉     | 107/215 [02:29<02:29,  1.38s/it]\u001B[A\n",
      " 50%|█████     | 108/215 [02:30<02:28,  1.39s/it]\u001B[A\n",
      " 51%|█████     | 109/215 [02:32<02:26,  1.38s/it]\u001B[A\n",
      " 51%|█████     | 110/215 [02:33<02:27,  1.40s/it]\u001B[A\n",
      " 52%|█████▏    | 111/215 [02:35<02:25,  1.40s/it]\u001B[A\n",
      " 52%|█████▏    | 112/215 [02:36<02:23,  1.39s/it]\u001B[A\n",
      " 53%|█████▎    | 113/215 [02:37<02:21,  1.39s/it]\u001B[A\n",
      " 53%|█████▎    | 114/215 [02:39<02:20,  1.39s/it]\u001B[A\n",
      " 53%|█████▎    | 115/215 [02:40<02:18,  1.39s/it]\u001B[A\n",
      " 54%|█████▍    | 116/215 [02:41<02:17,  1.38s/it]\u001B[A\n",
      " 54%|█████▍    | 117/215 [02:43<02:16,  1.40s/it]\u001B[A\n",
      " 55%|█████▍    | 118/215 [02:44<02:14,  1.39s/it]\u001B[A\n",
      " 55%|█████▌    | 119/215 [02:46<02:11,  1.37s/it]\u001B[A\n",
      " 56%|█████▌    | 120/215 [02:47<02:09,  1.36s/it]\u001B[A\n",
      " 56%|█████▋    | 121/215 [02:48<02:06,  1.35s/it]\u001B[A\n",
      " 57%|█████▋    | 122/215 [02:50<02:07,  1.37s/it]\u001B[A\n",
      " 57%|█████▋    | 123/215 [02:51<02:05,  1.37s/it]\u001B[A\n",
      " 58%|█████▊    | 124/215 [02:52<02:04,  1.37s/it]\u001B[A\n",
      " 58%|█████▊    | 125/215 [02:54<02:03,  1.37s/it]\u001B[A\n",
      " 59%|█████▊    | 126/215 [02:55<02:01,  1.37s/it]\u001B[A\n",
      " 59%|█████▉    | 127/215 [02:56<02:00,  1.37s/it]\u001B[A\n",
      " 60%|█████▉    | 128/215 [02:58<01:58,  1.37s/it]\u001B[A\n",
      " 60%|██████    | 129/215 [02:59<01:57,  1.36s/it]\u001B[A\n",
      " 60%|██████    | 130/215 [03:01<01:55,  1.36s/it]\u001B[A\n",
      " 61%|██████    | 131/215 [03:02<01:54,  1.37s/it]\u001B[A\n",
      " 61%|██████▏   | 132/215 [03:03<01:53,  1.36s/it]\u001B[A\n",
      " 62%|██████▏   | 133/215 [03:05<01:51,  1.36s/it]\u001B[A\n",
      " 62%|██████▏   | 134/215 [03:06<01:49,  1.36s/it]\u001B[A\n",
      " 63%|██████▎   | 135/215 [03:07<01:47,  1.35s/it]\u001B[A\n",
      " 63%|██████▎   | 136/215 [03:09<01:46,  1.35s/it]\u001B[A\n",
      " 64%|██████▎   | 137/215 [03:10<01:44,  1.34s/it]\u001B[A\n",
      " 64%|██████▍   | 138/215 [03:11<01:44,  1.35s/it]\u001B[A\n",
      " 65%|██████▍   | 139/215 [03:13<01:42,  1.35s/it]\u001B[A\n",
      " 65%|██████▌   | 140/215 [03:14<01:42,  1.37s/it]\u001B[A\n",
      " 66%|██████▌   | 141/215 [03:16<01:43,  1.40s/it]\u001B[A\n",
      " 66%|██████▌   | 142/215 [03:17<01:40,  1.38s/it]\u001B[A\n",
      " 67%|██████▋   | 143/215 [03:18<01:40,  1.39s/it]\u001B[A\n",
      " 67%|██████▋   | 144/215 [03:20<01:38,  1.38s/it]\u001B[A\n",
      " 67%|██████▋   | 145/215 [03:21<01:36,  1.38s/it]\u001B[A\n",
      " 68%|██████▊   | 146/215 [03:22<01:35,  1.39s/it]\u001B[A\n",
      " 68%|██████▊   | 147/215 [03:24<01:34,  1.39s/it]\u001B[A\n",
      " 69%|██████▉   | 148/215 [03:25<01:32,  1.39s/it]\u001B[A\n",
      " 69%|██████▉   | 149/215 [03:27<01:30,  1.37s/it]\u001B[A\n",
      " 70%|██████▉   | 150/215 [03:28<01:29,  1.37s/it]\u001B[A\n",
      " 70%|███████   | 151/215 [03:29<01:28,  1.38s/it]\u001B[A\n",
      " 71%|███████   | 152/215 [03:31<01:26,  1.38s/it]\u001B[A\n",
      " 71%|███████   | 153/215 [03:32<01:26,  1.39s/it]\u001B[A\n",
      " 72%|███████▏  | 154/215 [03:34<01:25,  1.40s/it]\u001B[A\n",
      " 72%|███████▏  | 155/215 [03:35<01:23,  1.40s/it]\u001B[A\n",
      " 73%|███████▎  | 156/215 [03:36<01:22,  1.40s/it]\u001B[A\n",
      " 73%|███████▎  | 157/215 [03:38<01:20,  1.40s/it]\u001B[A\n",
      " 73%|███████▎  | 158/215 [03:39<01:18,  1.39s/it]\u001B[A\n",
      " 74%|███████▍  | 159/215 [03:41<01:17,  1.39s/it]\u001B[A\n",
      " 74%|███████▍  | 160/215 [03:42<01:16,  1.40s/it]\u001B[A\n",
      " 75%|███████▍  | 161/215 [03:43<01:15,  1.39s/it]\u001B[A\n",
      " 75%|███████▌  | 162/215 [03:45<01:13,  1.39s/it]\u001B[A\n",
      " 76%|███████▌  | 163/215 [03:46<01:13,  1.40s/it]\u001B[A\n",
      " 76%|███████▋  | 164/215 [03:47<01:10,  1.39s/it]\u001B[A\n",
      " 77%|███████▋  | 165/215 [03:49<01:08,  1.38s/it]\u001B[A\n",
      " 77%|███████▋  | 166/215 [03:50<01:08,  1.39s/it]\u001B[A\n",
      " 78%|███████▊  | 167/215 [03:52<01:06,  1.39s/it]\u001B[A\n",
      " 78%|███████▊  | 168/215 [03:53<01:04,  1.38s/it]\u001B[A\n",
      " 79%|███████▊  | 169/215 [03:54<01:03,  1.38s/it]\u001B[A\n",
      " 79%|███████▉  | 170/215 [03:56<01:02,  1.39s/it]\u001B[A\n",
      " 80%|███████▉  | 171/215 [03:57<01:00,  1.38s/it]\u001B[A\n",
      " 80%|████████  | 172/215 [03:59<00:59,  1.38s/it]\u001B[A\n",
      " 80%|████████  | 173/215 [04:00<00:57,  1.37s/it]\u001B[A\n",
      " 81%|████████  | 174/215 [04:01<00:56,  1.38s/it]\u001B[A\n",
      " 81%|████████▏ | 175/215 [04:03<00:56,  1.40s/it]\u001B[A\n",
      " 82%|████████▏ | 176/215 [04:04<00:55,  1.41s/it]\u001B[A\n",
      " 82%|████████▏ | 177/215 [04:05<00:52,  1.38s/it]\u001B[A\n",
      " 83%|████████▎ | 178/215 [04:07<00:50,  1.36s/it]\u001B[A\n",
      " 83%|████████▎ | 179/215 [04:08<00:49,  1.37s/it]\u001B[A\n",
      " 84%|████████▎ | 180/215 [04:10<00:47,  1.36s/it]\u001B[A\n",
      " 84%|████████▍ | 181/215 [04:11<00:46,  1.36s/it]\u001B[A\n",
      " 85%|████████▍ | 182/215 [04:12<00:45,  1.39s/it]\u001B[A\n",
      " 85%|████████▌ | 183/215 [04:14<00:44,  1.38s/it]\u001B[A\n",
      " 86%|████████▌ | 184/215 [04:15<00:43,  1.39s/it]\u001B[A\n",
      " 86%|████████▌ | 185/215 [04:16<00:41,  1.38s/it]\u001B[A\n",
      " 87%|████████▋ | 186/215 [04:18<00:39,  1.37s/it]\u001B[A\n",
      " 87%|████████▋ | 187/215 [04:19<00:38,  1.38s/it]\u001B[A\n",
      " 87%|████████▋ | 188/215 [04:21<00:37,  1.37s/it]\u001B[A\n",
      " 88%|████████▊ | 189/215 [04:22<00:36,  1.39s/it]\u001B[A\n",
      " 88%|████████▊ | 190/215 [04:23<00:34,  1.36s/it]\u001B[A\n",
      " 89%|████████▉ | 191/215 [04:25<00:32,  1.36s/it]\u001B[A\n",
      " 89%|████████▉ | 192/215 [04:26<00:31,  1.36s/it]\u001B[A\n",
      " 90%|████████▉ | 193/215 [04:27<00:30,  1.37s/it]\u001B[A\n",
      " 90%|█████████ | 194/215 [04:29<00:28,  1.35s/it]\u001B[A\n",
      " 91%|█████████ | 195/215 [04:30<00:26,  1.35s/it]\u001B[A\n",
      " 91%|█████████ | 196/215 [04:31<00:25,  1.36s/it]\u001B[A\n",
      " 92%|█████████▏| 197/215 [04:33<00:24,  1.37s/it]\u001B[A\n",
      " 92%|█████████▏| 198/215 [04:35<00:25,  1.47s/it]\u001B[A\n",
      " 93%|█████████▎| 199/215 [04:36<00:22,  1.44s/it]\u001B[A\n",
      " 93%|█████████▎| 200/215 [04:37<00:21,  1.42s/it]\u001B[A\n",
      " 93%|█████████▎| 201/215 [04:39<00:19,  1.40s/it]\u001B[A\n",
      " 94%|█████████▍| 202/215 [04:40<00:18,  1.39s/it]\u001B[A\n",
      " 94%|█████████▍| 203/215 [04:41<00:16,  1.38s/it]\u001B[A\n",
      " 95%|█████████▍| 204/215 [04:43<00:14,  1.36s/it]\u001B[A\n",
      " 95%|█████████▌| 205/215 [04:44<00:13,  1.35s/it]\u001B[A\n",
      " 96%|█████████▌| 206/215 [04:45<00:12,  1.35s/it]\u001B[A\n",
      " 96%|█████████▋| 207/215 [04:47<00:10,  1.35s/it]\u001B[A\n",
      " 97%|█████████▋| 208/215 [04:48<00:09,  1.37s/it]\u001B[A\n",
      " 97%|█████████▋| 209/215 [04:49<00:08,  1.37s/it]\u001B[A\n",
      " 98%|█████████▊| 210/215 [04:51<00:06,  1.36s/it]\u001B[A\n",
      " 98%|█████████▊| 211/215 [04:52<00:05,  1.36s/it]\u001B[A\n",
      " 99%|█████████▊| 212/215 [04:54<00:04,  1.37s/it]\u001B[A\n",
      " 99%|█████████▉| 213/215 [04:55<00:02,  1.38s/it]\u001B[A\n",
      "100%|█████████▉| 214/215 [04:56<00:01,  1.38s/it]\u001B[A\n",
      "100%|██████████| 215/215 [04:57<00:00,  1.14s/it]\u001B[A\n",
      "  1%|          | 1/100 [04:57<8:10:45, 297.43s/it][A\n",
      "  0%|          | 0/215 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/215 [00:01<04:51,  1.36s/it]\u001B[A\n",
      "  1%|          | 2/215 [00:02<04:39,  1.31s/it]\u001B[A\n",
      "  1%|▏         | 3/215 [00:03<04:38,  1.31s/it]\u001B[A\n",
      "  2%|▏         | 4/215 [00:05<04:36,  1.31s/it]\u001B[A\n",
      "  2%|▏         | 5/215 [00:06<04:40,  1.34s/it]\u001B[A\n",
      "  1%|          | 1/100 [05:05<8:23:58, 305.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, targets)\n\u001B[0;32m     10\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 11\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     14\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\ia_2024\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    638\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    639\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    640\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    645\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    646\u001B[0m     )\n\u001B[1;32m--> 647\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    648\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ia_2024\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    349\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    351\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 354\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    362\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ia_2024\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    827\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 829\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    833\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Entrenam el classificador utilitzant transfer learning\n",
    "Utilitzam l'encoder de l'autoencoder entrenat com a extractor de característiques per a una tasca de classificació d'imatges. Congelam els pesos de l'encoder i afegim un nou model de classificació."
   ],
   "id": "c424035415cb06dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b148e-88ff-464d-859b-f1f252d86d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95665ce6-7995-43a9-bdde-e0fec1a8ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    torch.nn.Linear(8*8*512, 512),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(128, 4),\n",
    ") \n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset_cls = datasets.ImageFolder(root=data_dir, transform=input_transform)\n",
    "train_dataset_cls = torch.utils.data.Subset(train_dataset_cls, train_dss)\n",
    "train_dataloader_cls = DataLoader(train_dataset_cls, batch_size=batch_size, shuffle=True)"
   ],
   "id": "f7273397a97aa0cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenament del classificador",
   "id": "e316ad019fb40bce"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4699252-69d4-4c83-879b-0cca993b955b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e674784ac9487cae8f0aa0a8202422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49ce44739eb4525854e6e34620d89eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93789fa54de4f1891255561d526c2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.5437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91fd34efde94f429ef77efc97cdede0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.2955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84c3d6bc9354ae5b33e494378dfe0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.1870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9669de7de00407881fd1c7d825a1021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.0918\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in tqdm(range(5), desc=\"Èpoques\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in tqdm(train_dataloader_cls, desc=f\"Batches {epoch}\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08851f6b-00ca-42a6-a917-3b99f0f7d1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6712c4e686684622849474a1c1f3be5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30754276778962875 0.8848824786324786\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset_cls = torch.utils.data.Subset(datasets.ImageFolder(root=data_dir, transform=input_transform), test_dss)\n",
    "test_dataloader_cls = DataLoader(train_dataset_cls, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "total_loss = 0\n",
    "total_acc = 0\n",
    "model = model.eval()\n",
    "for inputs, targets in tqdm(test_dataloader_cls):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_acc += accuracy_score(torch.argmax(outputs, dim=1).cpu().detach().numpy(), targets.cpu().detach().numpy())\n",
    "    \n",
    "\n",
    "avg_loss = total_loss / len(train_loader)\n",
    "print(avg_loss, (total_acc / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a99588-8acd-4ad9-9c2f-fed8d53a0226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 0, 3, 1, 2, 3, 3, 0, 0, 3, 1, 1, 3, 1, 0, 2, 3, 2, 0, 2, 3, 3, 3, 3,\n",
       "         1, 0], device='cuda:0'),\n",
       " tensor([3, 0, 1, 1, 2, 0, 3, 0, 0, 3, 1, 1, 0, 1, 0, 2, 1, 2, 0, 2, 3, 3, 3, 1,\n",
       "         1, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs, dim=1), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e0e62e-bf70-4575-87b7-88998794df60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
